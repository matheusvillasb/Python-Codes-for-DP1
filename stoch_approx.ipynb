{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8487873a",
   "metadata": {},
   "source": [
    "# Stochastic Approximation\n",
    "\n",
    "If you haven't installed the quantecon Python library yet and you want to run this lecture, you need to execute the following line first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7729e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install quantecon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4107032",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this lecture we analyze a technique for optimizing functions and finding zeros (i.e., roots) of functions in settings where outcomes are only partially observable.\n",
    "\n",
    "The technique is called [stochastic approximation](https://en.wikipedia.org/wiki/Stochastic_approximation) and dates back to early work by [Munro and Robbins](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full).\n",
    "\n",
    "Stochastic approximation can be thought of as a generalization of several other famous algorithms, including stochastic gradient descent.\n",
    "\n",
    "It forms one of the key tools for training models built on neural networks, as well as in reinforcement learning.\n",
    "\n",
    "Here we focus on relatively simple problems, in order to convey the key ideas.\n",
    "\n",
    "We will use the following imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50750c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import quantecon as qe\n",
    "from numba import jit\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import bisect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3adbe9",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7f417f",
   "metadata": {},
   "source": [
    "As mentioned above, stochastic approximation helps us solve optimization problems and nonlinear equations in settings of uncertainty.\n",
    "\n",
    "Before we explain what it is, let's think about how it might be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab0df47",
   "metadata": {},
   "source": [
    "### A firm problem\n",
    "\n",
    "Suppose there is a firm that, each period, \n",
    "\n",
    "1. chooses inputs,\n",
    "2. produces,\n",
    "3. sells a certain number of its goods, and\n",
    "4. receives a corresponding profit.\n",
    "\n",
    "It wants to choose the input vector that maximizes expected profit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fd0d58",
   "metadata": {},
   "source": [
    "In Econ 101, we typically assume that the firm knows the exact relationship between inputs and outputs, as well as the full details of the demand function it faces.  \n",
    "\n",
    "But is this true in reality?  \n",
    "\n",
    "For example, the firm might be testing out a new product -- in which case it won't yet know exact production costs, or what the demand function will look like.\n",
    "\n",
    "How can the firm maximize expected profits in this setting?\n",
    "\n",
    "An econometrician might suggest that the firm \n",
    "\n",
    "1. makes some assumptions about functional forms, and\n",
    "2. tries to estimate profit / demand / supply relationships.\n",
    "\n",
    "But will these assumptions be valid?  \n",
    "\n",
    "Do we have data to accurately estimate all these relationships?\n",
    "\n",
    "Also, how can we get data on demand when the firm is selling a brand new product?\n",
    "\n",
    "(Econometricians have methods for handling missing data but -- again -- will their assumptions be valid?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fefa5d8",
   "metadata": {},
   "source": [
    "### A Monte Carlo solution\n",
    "\n",
    "Here's one \"model-free\" way the firm could deal with its optimization problem while avoiding questionable assumptions:\n",
    "\n",
    "1. Try many different combinations of inputs, recording resulting profits\n",
    "2. Choose the combination that achieved highest profits.\n",
    "  \n",
    "But this approach could be very costly!\n",
    "\n",
    "Not only would the firm have to try many input combinations, searching for the best one, they would also have to try each one of these combinations many times.\n",
    "\n",
    "This is because trying a fixed input vector $x$ onces is not enough, since the result profit is random.\n",
    "\n",
    "(Random disturbances push profits up and down in each period, even if inputs are fixed.)\n",
    "\n",
    "Each input vector $x$ would have to be tried many times in order to estimate expected profits at $x$.\n",
    "\n",
    "This means many trials $\\times$ many trials, potentially losing money all the while."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e362d3",
   "metadata": {},
   "source": [
    "### Stochastic approximation\n",
    "\n",
    "Stochastic approximation offers a way around this problem.\n",
    "\n",
    "It suggests an algorithm for updating the input vector $x$ in each period, based on the observed outcome in that period, which converges in probability to a point that maximizes expected profits.\n",
    "\n",
    "The algorithm is iterative and \"on-line\", meaning that the firm learns the best input vector while operating, rather than before starting  -- which is necessary in this case, since the firm needs to try different input vectors in order to learn about expected profits.\n",
    "\n",
    "Before we discuss such this application further, we start with some very simple mathematical examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507b085c",
   "metadata": {},
   "source": [
    "## Deterministic problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6d08c9",
   "metadata": {},
   "source": [
    "As a starting point, let's suppose that we are interested in finding the zero (i.e., root) of the function \n",
    "\n",
    "$$ \n",
    "    f(x) = \\ln x + \\exp(x/4) - \\tanh (x - 5) / 3 - 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f495f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return np.log(x) + np.exp(x / 4) - np.tanh(x - 5) / 3 - 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9cc36ea",
   "metadata": {},
   "source": [
    "Here's what the function looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b41a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.1, 10, 200)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, f(x), label='$f$')\n",
    "ax.plot(x, np.zeros(len(x)), 'k--')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41431eb9",
   "metadata": {},
   "source": [
    "Just from looking at the function, it seems like the zero is close to 3.5 ($x \\approx 3.5$ implies $f(x) \\approx 0$).\n",
    "\n",
    "But we want a more accurate measurement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01cd2d16",
   "metadata": {},
   "source": [
    "### A simple idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ecd7a",
   "metadata": {},
   "source": [
    "One we we could solve this problem numerically is by choosing some initial $x_0$ in the displayed region and then updating by\n",
    "\n",
    "$$ x_{n+1} = x_n - \\alpha_n f(x_n) $$\n",
    "\n",
    "where $(\\alpha_n)$ is a positive sequence converging to zero.\n",
    "\n",
    "In other words, we take a\n",
    "\n",
    "* small step up when $f(x_n) < 0$ and\n",
    "* a small step down when $f(x_n) > 0$.\n",
    "\n",
    "The size of the steps will decrease as we get closer to the zero of $f$ because \n",
    "\n",
    "1. $|f(x_n)|$ is (hopefully) getting close to zero, and\n",
    "2. $\\alpha_n$ is converging to zero.\n",
    "\n",
    "We will call this routine **stochastic approximation**, even though no aspect of the routine is stochastic at this stage.\n",
    "\n",
    "(Later we'll use an analogous method for stochastic problems.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70abc615",
   "metadata": {},
   "source": [
    "Here's an implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62703b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s_approx(f, x_0=8, tol=1e-6, max_iter=10_000):\n",
    "    x = x_0\n",
    "    error, n = tol + 1, 0\n",
    "    while n < max_iter and error > tol:\n",
    "        α = (n + 1)**(-.5)\n",
    "        x_new = x - α * f(x)\n",
    "        error = abs(x_new - x)\n",
    "        x = x_new\n",
    "        n += n\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e0e7f7",
   "metadata": {},
   "source": [
    "The routine seems to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_approx(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb54ea0d",
   "metadata": {},
   "source": [
    "You should be able to confirm that in the code above that,\n",
    "\n",
    "* convergence holds for for other initial conditions not too far from the root of $f$, and\n",
    "* a constant learning rate $\\alpha$ also works (can you see why?)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b00ad58",
   "metadata": {},
   "source": [
    "### Special cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41e175",
   "metadata": {},
   "source": [
    "The stochastic approximation update rule\n",
    "\n",
    "$$ x_{n+1} = x_n - \\alpha_n f(x_n) $$\n",
    "\n",
    "includes some important routines as special case.\n",
    "\n",
    "One is [Newton iteration](https://en.wikipedia.org/wiki/Newton%27s_method), which updates according to\n",
    "\n",
    "$$ x_{n+1} = x_n - (1/f'(x_n)) f(x_n) $$\n",
    "\n",
    "We can see that Newton iteration is a version of stochastic approximation with a special choice of learning rate.\n",
    "\n",
    "(In this case, we require that $f$ is increasing so that the learning rate $\\alpha_n = 1/f'(x_n)$ is positive.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca76f1dc",
   "metadata": {},
   "source": [
    "Another special case of stochastic approximation is **gradient descent**.\n",
    "\n",
    "Indeed, suppose $F$ is a differentiable real function and we seek the minimum of this function (assuming it exists).\n",
    "\n",
    "Gradient descent tells us to choose an initial guess $x_0$ and then update according to \n",
    "\n",
    "$$ x_{n+1} = x_n - \\alpha_n f(x_n) \n",
    "    \\quad \\text{where} \\quad\n",
    "    f := F'\n",
    "$$\n",
    "\n",
    "The sequence $(\\alpha_n)$ is a given learning rate.\n",
    "\n",
    "We see immediately that this is a special case of stochastic approximation when the zero we seek is a zero of a derivative of a function one wishes to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45142c7b",
   "metadata": {},
   "source": [
    "### Decreasing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4012b3",
   "metadata": {},
   "source": [
    "The stochastic approximation routine discussed above is aimed at handling increasing functions, since we take a small step down when $f(x_n)$ is positive and a small step up when $f(x_n)$ is negative.\n",
    "\n",
    "When the function in question is decreasing we need to reverse the sign.\n",
    "\n",
    "For example, suppose now that\n",
    "\n",
    "\n",
    "$$ \n",
    "    f(x) = 10 - \\ln x - \\exp(x/4) + \\tanh (x - 5) / 3 - 4\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba15b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 10 - np.log(x) - np.exp(x / 4) + np.tanh(x - 5) / 3 - 4\n",
    "    \n",
    "x = np.linspace(0.1, 10, 200)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, f(x), label='$f$')\n",
    "ax.plot(x, np.zeros(len(x)), 'k--')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74bf3bf",
   "metadata": {},
   "source": [
    "Now we set\n",
    "\n",
    "$$ x_{n+1} = x_n + \\alpha_n f(x_n) $$\n",
    "\n",
    "where, as before, $(\\alpha_n)$ is a positive sequence converging to zero.\n",
    "\n",
    "Since $f(x_n)$ is positive when $x_n$ is low, we take a small step up and continue.\n",
    "\n",
    "Similarly, when $x_n$ is above the root, we have $f(x_n) < 0$ and $x_{n+1}$ will be smaller.\n",
    "\n",
    "This leads to convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aebbb5",
   "metadata": {},
   "source": [
    "**Exercise:** \n",
    "\n",
    "Modify the stochastic approximation code above to obtain an approximation of the zero of $f$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4861c4",
   "metadata": {},
   "source": [
    "### An ODE perspective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f785e0f",
   "metadata": {},
   "source": [
    "There is an ODE (ordinary differential equation) perspective on the stochastic approximation routine that provides another way to analyze the routine.\n",
    "\n",
    "To see this, consider the scalar autonomous ODE\n",
    "\n",
    "$$ \\dot x = f(x) $$\n",
    "\n",
    "To approximate the trajectory of associated with these dynamics we can use a standard numerical scheme, which starts with the standard first order approximation for a differentiable function:  for small $h$,\n",
    "\n",
    "$$ x(t+h) \\approx x(t) + h x'(t) $$\n",
    "\n",
    "This suggests constructing a discrete trajectory via\n",
    "\n",
    "$$ x_{n+1} = x_n + h f(x_n), $$\n",
    "\n",
    "which is the same as our stochastic approximation rule for decreasing functions.\n",
    "\n",
    "To understand why this matches stochastic approximation for decreasing rather than increasing functions, note that when $f$ is strictly decreasing and has a unique zero $x^*$ in the interval $(a, b)$, any solution to the ODE converges to $x^*$.\n",
    "\n",
    "* For $x(t) < x^*$, we have $f(x(t))>0$ and hence $x(t)$ increases towards $x^*$.\n",
    "* For $x(t) > x^*$, we have $f(x(t))<0$, and hence $x(t)$ decreases towards $x^*$.\n",
    "\n",
    "Thus, the decreasing case corresponds to the setting where the zero $x^*$ is attracting for the ODE.\n",
    "\n",
    "This means that, for this stable case, the stochastic approximation rule $x_{n+1} = x_n + \\alpha_n f(x_n)$ converges to the stationary point of the ODE $\\dot x = f(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6d69904",
   "metadata": {},
   "source": [
    "## Stochastic Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed55810",
   "metadata": {},
   "source": [
    "As the name suggestions, stochastic approximation is really about solving problems that have random components.\n",
    "\n",
    "For example, let's suppose that we want to find the zero of $f$ but we cannot directly observe $f(x)$ at arbitrary $x$.\n",
    "\n",
    "Instead, after input $x_n$, we only observe a draw from $f(x_n) + D_n$, where $D_n$ is an independent draw from a fixed distribution $\\phi$ with zero mean.\n",
    "\n",
    "* we observe the sum $f(x_n) + D_n$ but cannot directly observe $f(x_n)$\n",
    "\n",
    "How should we proceed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969d64b1",
   "metadata": {},
   "source": [
    "Drawing a direct analogy with the deterministic case, and assuming again that $f$ is an increasing function, we set\n",
    "\n",
    "$$ \n",
    "    x_{n+1} = x_n - \\alpha_n (f(x_n) + D_n) \n",
    "$$\n",
    "\n",
    "Following [Monro and Robbins](https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-3/A-Stochastic-Approximation-Method/10.1214/aoms/1177729586.full), we require that the learning rate obeys\n",
    "\n",
    "$$ \n",
    "    \\sum_n \\alpha_n = \\infty \\quad \\text{and} \\quad \\sum_n \\alpha_n^2 < \\infty \n",
    "$$\n",
    "\n",
    "In particular, we will take $\\alpha_n = (n+1)^{-0.6}$\n",
    "\n",
    "We'll use the same function $f$ we used at the start, so that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afeb9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def f(x):\n",
    "    return np.log(x) + np.exp(x / 4) - np.tanh(x - 5) / 3 - 4\n",
    "    \n",
    "x = np.linspace(0.1, 10, 200)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, f(x), label='$f$')\n",
    "ax.plot(x, np.zeros(len(x)), 'k--')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f614316",
   "metadata": {},
   "source": [
    "This means that the zero we are searching for is approximately $x=3.557$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70338ddb",
   "metadata": {},
   "source": [
    "The shocks will be normal with zero mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f20678",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def generate_draw(x, σ=0.5):\n",
    "    return f(x) + σ * np.random.randn()\n",
    "\n",
    "@jit\n",
    "def s_approx_beta(x_0=5.0, num_iter=1_000):\n",
    "    x_sequence = np.empty(num_iter)\n",
    "    x = x_0\n",
    "    for n in range(num_iter):\n",
    "        x_sequence[n] = x\n",
    "        α = (n + 1)**(-0.6)\n",
    "        x = x - α * generate_draw(x)\n",
    "    return x_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc763ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = s_approx_beta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549efd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = 3.557\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, label='$(x_n)$')\n",
    "ax.plot(range(len(x)), np.full(len(x), x_star), 'k--', label='$x^*$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe65de3",
   "metadata": {},
   "source": [
    "We see that the sequence converges quite well to $x^* = 3.557$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620b7c1",
   "metadata": {},
   "source": [
    "Here's a function that simply returns the final estimate, rather than storing the whole sequence.\n",
    "\n",
    "We'll run it at a higher number of iterations and see if the final estimate is close to 3.557."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd543c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit\n",
    "def s_approx(x_0=5.0, num_iter=10_000_000):\n",
    "    x = x_0\n",
    "    for n in range(num_iter):\n",
    "        α = (n + 1)**(-.6)\n",
    "        x = x - α * generate_draw(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a5079c",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_approx()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802dd899",
   "metadata": {},
   "source": [
    "The result is pretty good.\n",
    "\n",
    "For more on the convergence properties of of stochastic approximation, see, e.g., [this reference](https://arxiv.org/abs/2205.01303).\n",
    "\n",
    "Note that the routine looks essentially the same in higher dimensions: when $f$ maps $\\mathbb R^k$ to itself and each $D_k$ is a random vector in $\\mathbb R^k$, we choose an initial $k$-vector $x_0$ and update via\n",
    "\n",
    "$$ x_{n+1} = x_n - \\alpha_n (f(x_n) + D_n)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e551b3f",
   "metadata": {},
   "source": [
    "## Application: profit maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3b16be",
   "metadata": {},
   "source": [
    "Now let's go back to the problem involving maximization of expected profit that we discussed at the start of the lecture.\n",
    "\n",
    "We'll take on a toy version of this problem with just one input --- let's say it's labor.\n",
    "\n",
    "Profits take the form\n",
    "\n",
    "$$ \\pi_n = \\pi(x_n, \\xi_n) $$\n",
    "\n",
    "where $x_n$ is a value in $(0, \\infty)$ denoting current labor input and $\\xi_n$ is an independent draw from a fixed distribution $\\phi$ on $\\mathbb R$.\n",
    "\n",
    "The firm manager hopes to maximize \n",
    "\n",
    "$$\n",
    "    \\bar \\pi(x) := \\mathbb E \\, \\pi(x, \\xi) = \\int \\pi(x, z) \\phi(dz)\n",
    "$$\n",
    "\n",
    "The manager doesn't know the function $\\pi$ or the distribution $\\phi$.\n",
    "\n",
    "However, the manager can observe profits in each period.\n",
    "\n",
    "To make our life a little easier, we'll also assume that the firm manager can observe local changes to profits.\n",
    "\n",
    "(Perhaps, during one period, the manager can slightly vary work hours up and down to get an estimate of marginal returns.)\n",
    "\n",
    "In particular, the manager can observe\n",
    "\n",
    "$$\n",
    "    \\Delta_n := \\pi^\\prime_1(x_n, \\xi_n) := \\frac{\\partial}{\\partial x} \\pi(x_n, \\xi_n),\n",
    "    \\qquad n = 0, 1, \\ldots\n",
    "$$\n",
    "\n",
    "Given a starting value of labor input, the firm now follows the update rule\n",
    "\n",
    "$$\n",
    "    x_{n+1} = x_n + \\alpha_n \\Delta_n\n",
    "$$\n",
    "\n",
    "where $\\alpha_n$ is the learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54596ee5",
   "metadata": {},
   "source": [
    "To experiment with these ideas, let's suppose that, unknown to the manager, \n",
    "\n",
    "* output obeys $q = q(x) = x^\\alpha$ for some $\\alpha \\in (0,1)$,\n",
    "* wages are constant at $w > 0$, and\n",
    "* the inverse demand curve obeys $p = \\xi \\exp(- q)$, where $\\ln \\xi \\sim N(\\mu, \\sigma)$.\n",
    "\n",
    "As a result, profits are given by \n",
    "\n",
    "$$\n",
    "    \\pi = p q - w x = \\xi \\exp(-x^\\alpha) x^\\alpha - w x\n",
    "$$\n",
    "\n",
    "and expected profits are \n",
    "\n",
    "$$\n",
    "    \\bar \\pi = \\bar \\xi \\exp(-x^\\alpha) x^\\alpha - wx,\n",
    "    \\qquad \\bar \\xi := \\mathbb E \\xi = \\exp\\left(\\mu + \\frac{\\sigma^2}{2} \\right).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5a52fe",
   "metadata": {},
   "source": [
    "Here's the function for expected profits, plotted at a default set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a3057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_pi(x, params):\n",
    "    α, w, μ, σ = params\n",
    "    bar_xi = np.exp(μ + σ**2 / 2)\n",
    "    return bar_xi * np.exp(-x**α) * x**α - w * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6d73ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_params = 0.85, 1.0, 1.0, 0.05  # α, w, μ, σ\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0.01, 1, 200)\n",
    "y = bar_pi(x, default_params)\n",
    "ax.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1f5e8e",
   "metadata": {},
   "source": [
    "Here's the expectation of the gradient of the profit function, derived from the calculations above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_bar_pi(x, params):\n",
    "    α, w, μ, σ = params\n",
    "    bar_xi = np.exp(μ + σ**2 / 2)\n",
    "    return bar_xi * α * np.exp(-x**α) * (x**(α - 1) - x**(2*α - 1)) - w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c05997",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = np.linspace(0.01, 1, 200)\n",
    "y = diff_bar_pi(x, default_params)\n",
    "ax.plot(x, y)\n",
    "ax.plot(x, 0 * x, 'k--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673f1014",
   "metadata": {},
   "source": [
    "If we have all this information in hand, we can easily find the root of the derivative and hence the maximizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b8ccb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_star = bisect(lambda x: diff_bar_pi(x, default_params), 0.2, 0.6)\n",
    "x_star"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6801534",
   "metadata": {},
   "source": [
    "Now let's go back to the setting where we only get to see observations and need to apply stochastic approximation.\n",
    "\n",
    "Here's a function to generate draws from the gradient\n",
    "\n",
    "$$ \n",
    " \\frac{\\partial}{\\partial x} \\pi(x_n, \\xi_n)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75881646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_gradient_draw(x, params):\n",
    "    α, w, μ, σ = params\n",
    "    # Generate a draw of ξ_n\n",
    "    xi = np.exp(μ + σ * np.random.randn())\n",
    "    # Return the observation of the local gradient\n",
    "    return xi * α * np.exp(-x**α) * (x**(α - 1) - x**(2*α - 1)) - w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eacfeb",
   "metadata": {},
   "source": [
    "Here's a routine for stochastic approximation, similar to the ones above.\n",
    "\n",
    "This routine stores the sequence $(x_n)$, so we can plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d3879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_approx(params=default_params, x_0=0.1, num_iter=40):\n",
    "    x_sequence = np.empty(num_iter+1)\n",
    "    x = x_0\n",
    "    x_sequence[0] = x\n",
    "    for n in range(num_iter):\n",
    "        α = 1 / (n + 1)\n",
    "        x = x + α * generate_gradient_draw(x, params)\n",
    "        x_sequence[n+1] = x\n",
    "    return x_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64768ef9",
   "metadata": {},
   "source": [
    "Let's generate and plot the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f60b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = stochastic_approx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033faabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, label='$(x_n)$')\n",
    "ax.plot(range(len(x)), np.full(len(x), x_star), 'k--', label='$x^*$')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700f27f7",
   "metadata": {},
   "source": [
    "In this case, the convergence is quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb63c43",
   "metadata": {},
   "source": [
    "## Concluding comments\n",
    "\n",
    "In this lecture we described the stochastic approximation algorithm and studied its behavior in some applications.\n",
    "\n",
    "One important application of stochatic approximation in the field of artificial intelligence is reinforcement learning.\n",
    "\n",
    "We study these connections in a separate lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a79f04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
